
1.importing all tables from RDBMS to hadoop:
============================================

sqoop import-all-table \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password root \
--warhouse-dir 'user/data/sq_tables/' \
--autoreset-to-one-mapper ;



2.Exclude tables from RDBMS to sqoop:
=====================================
sqoop import-all-table \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password root \
--exclude-table 'orders,customers' \
--warhouse-dir 'user/data/sq_tables/' \
--autoreset-to-one-mapper ;

Note:
-----
when we are importing all tables ,for any one of table has no primary key then we need to use --autoreset-to-one-mapper,can't use --split-by



3.import data from rdbms table to hive table:
=============================================

sqoop 
--options-file D:/Big_Data/Sqoop/code/sqoop-connect 
--table orders \
--hive-import \
--hive-database hive_sq \
--hive-table hive_sq_orders \
-- m 1 ;



3.sqoop incremental/delta loads (append,check column,last value):
=================================================================

load historical data:(only once)
---------------------

sqoop
--option-file D:/Big_Data/Sqoop/code/sqoop-connect \
--table order \
--target-dir /user/coludera/retail/ \
--delete-target-dir \
--fields-terminated-by ',' \
--lines-terminated-by ',' \
-m 3 ;

to get updated records:(every day)
----------------------------

sqoop import \
--option-file D:/Big_Data/Sqoop/code/sqoop-connect \
--table order \
--target-dir /user/coludera/retail/ \
--incremental append \
--check-column order_id \
--last-value '2023-05-01' \
--fields-terminated-by ',' \
--lines-terminated-by ',' \
-m 3 ;
